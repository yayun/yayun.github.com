---
layout: post
title: "MIT 6.824 Lab2 笔记"
---

MIT6.824 lab2 分为 3 个 task: 2A 需要实现 leader 选举和 heartbeat（appendEntries 不涉及 log entries）; 2B 需要实现涉及日志复制的问题；2C 实现持久化设计的部分。

总之，调 bug 调到怀疑人生 🤦‍♀️...

借用 mit6.824 课程中助教的一段话，与君共勉：Focus on how to write correct code, dont't focus way too much on the happens before relation and being able to reason about exactly why incorrect code is't correct like we done't really care, we just want to be able to write correct code and call it a day.

Raft 一共只有两个接口：AppendEntries 和 RequestVote。听起来很简单，但是里面的细节非常多。一定要仔细看论文（尤其是论文中的 figure2）和 student's guide。

### Raft 数据结构

	type Raft struct {
		mu        sync.Mutex          // Lock to protect shared access to this peer's state
		peers     []*labrpc.ClientEnd // RPC end points of all peers
		persister *Persister          // Object to hold this peer's persisted state
		me        int                 // this peer's index into peers[]
		dead      int32               // set by Kill()

		votedFor int // 当前任期内投给了谁

		// Persistent state on all servers
		currentTerm int
		logEntries  []logEntry // log entries; each entry contains command for state machine, and term when entry was received by leader (first index is 1)

		// Volatile state on all servers
		commitIndex int // index of highest log entry known to be committed
		lastApplied int // index of highest log entry applied to state machine

		// Volatile state on leaders
		nextIndex  []int // for each server, index of the next log entry to send to that server (initialized to leader last log index + 1)
		matchIndex []int // for each server, index of highest log entry known to be replicated on server (initialized to 0, increases monotonically)

		currentRole            int              // 当前 server 的身份
		votes                  map[int]bool // 选举的得票数
		electionTimeout        int32            // 选举超时时间
		heartbeatTimestamp     int32            // 收到心跳的时间
		lastHeartbeatTimestamp int32            // 上次发出心跳的时间
		applyCh chan ApplyMsg
	}
	
### Leader 选举
Make 函数创建 raft server，设置 electionTimeout 和 心跳时间， electionTimeout 随机，目前我设置的是 300ms～400ms。`raftServer()` , 每隔 `DefaultIntervalMs` 检测当前节点，是否要发起选举（follower or candidate），是否要发起 appendEntries（leader）。DefaultIntervalMs 我设置了 10ms。

	func Make(peers []*labrpc.ClientEnd, me int,
	persister *Persister, applyCh chan ApplyMsg) *Raft {
		rf := &Raft{}
		rf.peers = peers
		rf.persister = persister
		rf.me = me
		rf.currentRole = Follower
		rf.electionTimeout = rf.getElectionTimeout()
		rf.heartbeatTimestamp = int32(time.Now().UnixNano() / 1e6)
		rf.lastHeartbeatTimestamp = 0
		rf.lastApplied = 0
		rf.commitIndex = 0
		rf.applyCh = applyCh
		rf.votedFor = -1
	
		// Your initialization code here (2A, 2B, 2C).
		// create a goroutine
		go rf.raftServer()
		// initialize from state persisted before a crash
		rf.readPersist(persister.ReadRaftState())
		return rf
	}
	
如果当前节点是 candidate 或者 follower 并且上次收到心跳的时间（heartbeatTimestamp）距离现在超过了 electionTimeout 则发起选举。如果当前节点是 leader 并且上次心跳时间距离现在超过 DefaultHeartbeatIntervalMs （我设置的是 100ms）则发送 appendEntries RPC。

	func (rf *Raft) raftServer() {
		for {
			if rf.canStartElection() {
				rf.startSelection()
				rf.persist()
			}
			if rf.canSendHeartbeat() {
				rf.sendHeartbeat()
			}
			time.Sleep(DefaultIntervalMs * time.Millisecond)
		}
	}

**发起选举**：

1. 将自己的身份切换成 candidate
2. 当前 term++，给自己投一票
3. resetElectionTimer
4. 同时往其他节点发起投票请求。

发起选举相关代码：

	// 发起投票
	func (rf *Raft) startSelection() {
		lastLogIndex, lastLogTerm := rf.getLastLogIndexAndTerm()
		rf.mu.Lock()
		rf.currentRole = Candidate
		rf.votes = make(map[int]bool)
		rf.votes[rf.me] = true
		rf.currentTerm ++
		rf.votedFor = rf.me
		candidateId := rf.me
		currentRole := rf.currentRole
		peers := rf.peers
		currentTerm := rf.currentTerm
		me := rf.me
		rf.mu.Unlock()
		rf.resetElectionTimer()
		for server, _ := range peers {
			if server == candidateId || currentRole != Candidate {
				continue
			}
			reply := &RequestVoteReply{}
			args := &RequestVoteArgs{
				Term: currentTerm,
				CandidateId: me,
				LastLogIndex: lastLogIndex,
				LastLogTerm: lastLogTerm,
			}
			go rf.sendRequestVote(server, args, reply, peers)
		}
	}	


<img src="/images/requestVote.png">

发起投票之后每收到投票响应都判断下对方节点是否投票给自己，如果得到选票判断下是否得到大多数的选票，如果收到大多数的选票则 becomeLeader。

	func (rf *Raft) sendRequestVote(server int, args * RequestVoteArgs, reply *RequestVoteReply, peers []*labrpc.ClientEnd) {
		peer := peers[server]
		ok := peer.Call("Raft.RequestVote", args, reply)
		if !ok {
			return
		}
		rf.convertToFollowerOrNot(reply.Term, server)
		rf.mu.Lock()
		currentRole := rf.currentRole
		rf.mu.Unlock()
		if currentRole == Follower {
			return
		}
		if reply.VoteGranted {
			rf.mu.Lock()
			rf.votes[server] = true
			votes := rf.votes
			rf.mu.Unlock()
			if len(votes) >= rf.majorityCount() && currentRole != Leader {
				rf.becomeLeader()
			}
		}
	}

节点收到 candidate 的投票请求

1. Reply false if term < currentTerm
2. If votedFor is null or candidateId, and candidate’s log is at least as up-to-date as receiver’s log, grant vote 

1.如果 term < currentTerm 直接返回失败；2. 如果当前 term （candidate 发过来的参数）下已经给其他节点投票返回失败；如果当前 term 下没有给其他节点投过票或者已经给当前 candidate 投过票，则判断 candidate 的日志：
a. candidate 最后一条日志的 term 如果大于自己最后一条日志的 term 则返回成功；
b. candidate 最后一条日志的 term 和自己最后一条日志的 term 一样大情况，比较最后一条日志的 index 如果 candidate 最后一条日志的 index 大于自己最后一条日志的 index 则返回成功。

RequestVote handler


	func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) {
		rf.convertToFollowerOrNot(args.Term, args.CandidateId)
		lastLogIndex, lastLogTerm := rf.getLastLogIndexAndTerm()
		rf.mu.Lock()
		reply.Term = rf.currentTerm
		reply.VoteGranted = false
		if args.Term < rf.currentTerm {
			reply.VoteGranted = false
		} else if rf.votedFor >= 0 && rf.votedFor != args.CandidateId && rf.currentTerm == args.Term {
			reply.VoteGranted = false
		} else if rf.votedFor < 0 || (rf.votedFor == args.CandidateId && rf.currentTerm == args.Term) || rf.currentTerm != args.Term {
			if args.LastLogTerm > lastLogTerm || (args.LastLogTerm == lastLogTerm && args.LastLogIndex >= lastLogIndex) {
				reply.VoteGranted = true
				rf.votedFor = args.CandidateId
			}
		}
		rf.mu.Unlock()
		if reply.VoteGranted {
			rf.persist()
			rf.resetElectionTimer()
		}
	}
	

Candidate 成为 leader 之后要往其他候选人发送心跳，来宣示自己的领导地位。

	func (rf * Raft) sendHeartbeat() {
		rf.mu.Lock()
		me := rf.me
		peers := rf.peers
		currentRole := rf.currentRole
		currentTerm := rf.currentTerm
		commitIndex := rf.commitIndex
		rf.lastHeartbeatTimestamp = rf.getCurrentMillTimeStamp()
		rf.mu.Unlock()
		for server, _ := range peers {
			if server == me || currentRole != Leader {
				continue
			}
			args := &AppendEntriesArgs{
				Term: currentTerm,
				LeaderId: me,
				LeaderCommit: commitIndex,
			}
			args.Entries, args.PrevLogIndex, args.PrevLogTerm = rf.getAppendEntriesArgsByServer(server)
			reply := &AppendEntriesReply{}
			go rf.appendEntries(server, args, reply)
		}

### 日志复制

leader 在收到日志后会先把日志写到本地的 logEntries 里面。

	func (rf *Raft) Start(command interface{}) (int, int, bool) {
		index := -1
		term := -1
		isLeader := true

		// Your code here (2B).
		_, isLeader = rf.GetState()
		if !isLeader {
			return index, term, false
		}
		index, term = rf.handleLogs(command)
		rf.persist()
		return index, term, isLeader
	}
	
	func (rf *Raft) handleLogs(command interface{}) (int, int){
		rf.mu.Lock()
		log := logEntry {
			Term: rf.currentTerm,
			Command: command,
		}
		rf.logEntries = append(rf.logEntries, log)
		me := rf.me
		lastLogIndex := len(rf.logEntries)
		currentTerm := rf.currentTerm
		rf.mu.Unlock()
		rf.updateNextIndexAndMatchIndex(me, lastLogIndex)
		return lastLogIndex, currentTerm
		}

Leader 会不断的心跳顺带将自己的日志同步给 followers。
	
AppendEntries RPC

<img src="/images/appendEntries.png">

AppendEntries handler

	func (rf *Raft) AppendEntriesHandler(args *AppendEntriesArgs, reply *AppendEntriesReply) {
		rf.convertToFollowerOrNot(args.Term, args.LeaderId)
		rf.mu.Lock()
		reply.Term = rf.currentTerm
		if args.Term < rf.currentTerm {
			reply.Success = false
			rf.mu.Unlock()
			return
		}
		canSavePersistData = false
		reply.Success = true
		if args.PrevLogIndex > len(rf.logEntries) {
			reply.Success = false
			reply.ConflictIndex = len(rf.logEntries) + 1
			reply.ConflictTerm = -1
		} else if args.PrevLogIndex > 0 && rf.logEntries[args.PrevLogIndex - 1].Term != args.PrevLogTerm {
			reply.Success = false
			reply.ConflictTerm = rf.logEntries[args.PrevLogIndex - 1].Term
			for index, log := range rf.logEntries {
				if log.Term == reply.ConflictTerm {
					reply.ConflictIndex = index + 1
					break
				}
			}
			reply.ConflictTerm = -1
			rf.logEntries = rf.logEntries[:args.PrevLogIndex-1]
			canSavePersistData = true
		} else if args.PrevLogIndex == 0 {
			rf.logEntries = args.Entries
			canSavePersistData = true
		} else if rf.logEntries[args.PrevLogIndex-1].Term == args.PrevLogTerm && len(args.Entries) != 0 {
			rf.logEntries = append(rf.logEntries[:args.PrevLogIndex], args.Entries...)
			canSavePersistData = true
		}
		if reply.Success {
			index := int(math.Min(float64(args.LeaderCommit), float64(len(rf.logEntries))))
			if index > rf.commitIndex {
				rf.commitIndex = index
			}
		}
		rf.mu.Unlock()
		rf.resetElectionTimer()
		// 虽然没有成功 日志有可能已经被删除
		if canSavePersistData {
			rf.persist()
		}
		if reply.Success {
			rf.applyMsg()
		}	
	}

在写代码的时候曾经纠结一个问题：follower 有可能会先收到 leader 发来一条比较长的日志接着收到一条相对比较短的日志，这样 follower 的日志有可能先变长接着又变短。这样其实没问题，因为节点的 commitIndex increases
monotonically，就算日志变短，leader 收到 follower 的日志之后会根据 prevLogIndex + len(args.logEntries) 来更新 follower 的 matchIndex，这样 follower 的日志还是会最终一致起来的。

“For All servers， If commitIndex > lastApplied: increment lastApplied, apply log[lastApplied] to state machine”。appendEntries handler / appendEntries 收到 follower 的返回 commitIndex 可能发生了变化， 这两处都需要调用`applyMsg()` 函数：

	func (rf *Raft) applyMsg() {
		rf.mu.Lock()
		for rf.commitIndex > rf.lastApplied {
			rf.lastApplied ++
			msg := ApplyMsg{
				CommandValid: true,
				Command: rf.logEntries[rf.lastApplied-1].Command,
				CommandIndex: rf.lastApplied,
			}
			rf.applyCh <- msg
		}
		rf.mu.Unlock()
	}

AppendEntries 函数
	
	func (rf *Raft) appendEntries(server int, args *AppendEntriesArgs, reply *AppendEntriesReply) {
		rf.mu.Lock()
		peer := rf.peers[server]
		rf.mu.Unlock()
		ok := peer.Call("Raft.AppendEntriesHandler", args, reply)
		if !ok {
			return
		}
		rf.convertToFollowerOrNot(reply.Term, server)
		rf.mu.Lock()
		currentRole := rf.currentRole
		rf.mu.Unlock()
		if currentRole != Leader {
			return
		}
		if reply.Success {
			if len(args.Entries) == 0 {
				return
			}
			rf.updateNextIndexAndMatchIndex(server, args.PrevLogIndex + len(args.Entries))
			rf.updateLeaderCommitIndex()
			rf.applyMsg()
			return
		}
		rf.mu.Lock()
		rf.nextIndex[server] = reply.ConflictIndex
	if reply.ConflictTerm > 0 {
		for i := len(rf.logEntries) -1; i >= 0; i -- {
			if rf.logEntries[i].Term == reply.ConflictTerm {
				rf.nextIndex[server] = i + 2
			}
		}
	}
		rf.nextIndex[server] = int(math.Max(float64(rf.nextIndex[server]), float64(1)))
		rf.mu.Unlock()
	}

获取 AppendEntries 参数

	func (rf *Raft) getAppendEntriesArgsByServer(server int) ([]logEntry, int, int) {
		rf.mu.Lock()
		defer rf.mu.Unlock()
		nextIndex := rf.nextIndex[server]
		prevLogIndex := nextIndex - 1
		var logEntries []logEntry
		prevLogTerm := -1
		logEntries = rf.logEntries[prevLogIndex:]
		if prevLogIndex >= 1 {
			prevLogTerm = rf.logEntries[prevLogIndex-1].Term
		}
		return logEntries, prevLogIndex, prevLogTerm
	}

上一篇 raft 笔记中 *安全性约束* 中提到论文中 figure8 描述的复制前任 leader 未 commit 日志的问题。最初实现的时候，我在 leader 发起 appendEntries 之前都先判断下 leader 将要发送给 follower 的最后一条日志的 term 是否等于当前 leader 的 term，如果不相等则将 args.Entries 设置为空，不发送日志。但是这样有一个问题：leader 当选之后有一些已经 commit 的日志有些节点还没有同步，但是 leader 一直也没有收到来自客户端的新日志，如果按照上述实现方式，leader 的这些日志一直没办法同步给这些节点。

具体实现方式是在 leader 当选之后立即写入一个当前 term 的 no-op 日志。但是 2B 中的一些 testcase 是检查日志 index 的，一些 testcase 会失败。我曾经尝试不按照论文中的方式让日志的 index 从 0 开始，其他 testcase 会过但是 `TestFailNoAgree2B` case 还是不稳定，因为新的 leader 当选之后会写一条空日志，还是会打乱之前 index。最后我索性还是 index 从 1 开始，去掉了节点当选 leader 写入 no-op 日志。最后所有的 testcase 稳过。（怀疑人生... 决定先搁置，等空了再研究下 `TestFigure82C` `TestUnreliableAgree2C`）

### 几个重要细节


#### **matchIndex and nextIndex**

matchIndex 记录每个 server 当前日志同步到了哪里。所以每次节点日志更新成功都需要更新 matchIndex，同时更新 nextIndex = matchIndex + 1

	func (rf * Raft) updateNextIndexAndMatchIndex(server int, matchIndex int) {
		rf.mu.Lock()
		defer rf.mu.Unlock()
		if matchIndex <= rf.matchIndex[server] {
			return
		}
		rf.matchIndex[server] = matchIndex
		rf.nextIndex[server] = rf.matchIndex[server] + 1
		return
	}
	
matchIndex 是一个最终结果，nextIndex 是一个中间结果。

如果 follower 同步日志返回成功，则更新这个 follower 的 matchIndex，matchIndex = prevLogIndex + len(entries[])， nextIndex = matchIndex + 1。

如果 follow 同步日志返回失败（不是因为 term 比对方小导致的失败），则一直 decrements nextIndex，直到双方日志达到一个匹配的点。

我最初实现的版本按照论文中写的每次日志匹配失败 nextIndex 都减1，但是在跑 2C Figure8 Unreliable 的时候一直报 `Failed to reach agreement` 错误，按照 `students' guide` 提及的方法优化，减少 `rejected AppendEntries RPCs` 的次数。

>If desired, the protocol can be optimized to reduce the
number of rejected AppendEntries RPCs. For example,
when rejecting an AppendEntries request, the follower can include the term of the conflicting entry and the first
index it stores for that term. With this information, the
leader can decrement nextIndex to bypass all of the conflicting entries in that term; one AppendEntries RPC will
be required for each term with conflicting entries, rather
than one RPC per entry. In practice, we doubt this optimization is necessary, since failures happen infrequently
and it is unlikely that there will be many inconsistent entries.

For follower 

>1. If a follower does not have prevLogIndex in its log, it should return with conflictIndex = len(log) and conflictTerm = None.
2. If a follower does have prevLogIndex in its log, but the term does not match, it should return conflictTerm = log[prevLogIndex].Term, and then search its log for the first index whose entry has term equal to conflictTerm.

For Leader 

Leader 收到 follower 返回的 conflictTerm，需要在自己的日志中找到与 conflictTerm 相等的最后一条日志，将 nextIndex 设置成这条日志的下一条日志的 index。 

> 1. Upon receiving a conflict response, the leader should first search its log for conflictTerm. 
> 2. If it finds an entry in its log with that term, it should set nextIndex to be the one beyond the index of the last entry in that term in its log.
If it does not find an entry with that term, it should set nextIndex = conflictIndex.

其实 leader 可以直接设置 nextIndex = conflictIndex, 引入 ConflictTerm 可以减少非必要的日志同步：

> A half-way solution is to just use conflictIndex (and ignore conflictTerm), which simplifies the implementation, but then the leader will sometimes end up sending more log entries to the follower than is strictly necessary to bring them up to date.

<img src="/images/confilctIndex.png">

如果仅仅用 conflictIndex 不用 conflictTerm。上图，最开始 leader nextIndex = 9， prevLogIndex = 8，prevLogTerm = 3，follower 收到日志，conflictTerm = 2，conflictIndex = 3。如果 leader 收到回复之后仅仅将 nextIndex 设置为 3，会将 Index = [3, 4] 两条不必要发送的日志也发送过去。如果有 conflictTerm = 2， nextIndex 为 5，阻止了 index = [3, 4] 两条不必要的日志发送。

matchIndex and nextIndex votes 初始化

	func (rf *Raft) becomeLeader() {
		lastLogIndex, _ := rf.getLastLogIndexAndTerm()
		rf.mu.Lock()
		rf.currentRole = Leader
		rf.nextIndex = make([]int, len(rf.peers))
		rf.matchIndex = make([]int, len(rf.peers))
		rf.votes = make(map[int]bool)
		for server, _ := range rf.peers {
			rf.nextIndex[server] = lastLogIndex + 1
		}
		rf.mu.Unlock()
		//rf.handleLogs("no-op")
		rf.persist()
	}


#### **CommitIndex**

**Leader**

Leader appendEntries 成功返回之后会根据 matchIndex 来更新 commitIndex：If there exists an N such that N > commitIndex, a majority of matchIndex[i] ≥ N, and log[N].term == currentTerm: set commitIndex = N。实现中对 matchIndex 按从高到低排序，取 index = majorityCount 处的 machINdex 为 commitIndex。

	func (rf *Raft) updateLeaderCommitIndex() {
		majorityCount := rf.majorityCount()
		rf.mu.Lock()
		var sortedMatchIndex = make([]int, len(rf.matchIndex))
		copy(sortedMatchIndex, rf.matchIndex)
	sort.Slice(sortedMatchIndex, func(i, j int) bool {
		return sortedMatchIndex[i] > sortedMatchIndex[j]
	})
		majorityCommitIndex := sortedMatchIndex[majorityCount-1]
		if majorityCommitIndex > rf.commitIndex {
			rf.commitIndex = majorityCommitIndex
		}
		rf.mu.Unlock()
	}

**follower**

If leaderCommit > commitIndex, set commitIndex =
min(leaderCommit, index of last new entry)。Follower 在 append 日志成功之后：
	
	if reply.Success {
		index := int(math.Min(float64(args.LeaderCommit), float64(len(rf.logEntries))))
		if index > rf.commitIndex {
			rf.commitIndex = index
		}
	}
	
#### **when convert to follower**

>Ensure that you follow the second rule in “Rules for Servers” before handling an incoming RPC. The second rule states:
>>**If RPC request or response contains term T > currentTerm: set currentTerm = T, convert to follower (§5.1)**

>For example, if you have already voted in the current term, and an incoming RequestVote RPC has a higher term that you, you should first step down and adopt their term (thereby resetting votedFor), and then handle the RPC, which will result in you granting the vote!

有四处地方需要判断是否需要 convertToFollower：1. 发起投票收到响应之后; 2. requestVote handler ；3. Leader 发送 appendEntries 收到响应之后；4. appendEntries handler。**before handling an incoming RPC** 这点很重要！

	func (rf *Raft) convertToFollowerOrNot(term int, server int) {
		rf.mu.Lock()
		if term > rf.currentTerm {
			rf.currentRole = Follower
			rf.currentTerm = term
			rf.votedFor = -1
		}
		rf.mu.Unlock()
		rf.persist()
	}

#### **Reset electionTimer**

> Make sure you reset your election timer exactly when Figure 2 says you should. Specifically, you should only restart your election timer if a) you get an AppendEntries RPC from the current leader (i.e., if the term in the AppendEntries arguments is outdated, you should not reset your timer); b) you are starting an election; or c) you grant a vote to another peer

重置 electionTimer 时机：

 1. 发起选举之后 
 2. requestVote hander 给某位 candidate 投票之后
 3. appendEntries handler 在判断 convertToFollower 之后


	func (rf *Raft) resetElectionTimer() {
		rf.mu.Lock()
		rf.electionTimeout = rf.getElectionTimeout()
		rf.heartbeatTimestamp = rf.getCurrentMillTimeStamp()
		rf.mu.Unlock()
	}

### 持久化

currentTerm votedFor log[] 这几个参数是需要持久化的数据，所以只需要在这些参数有可能发生变化的时候调用 rf.persist() 即可。
由于 commitIndex 和 nextIndex 是易失的数据，在某个节点当选为 leader 之后，commitIndex lastApplied 都重新初始化，这样会导致节点的日志会重复 apply。

**LogEntries**

It is the only record of the application
w state. When the server restarts the only information available to reconstruct the application state is the sequence of commands in the log

**CurrentTerm**

记录日志时需要记录当前日志的 term。
发起投票的时候需要比较两者的 term。
某个 term 下只能有 1 个 leader。

**votedFor**

假设一个节点在收到另一个节点的投票请求把票投给了他之后挂掉了。如果它没有持久化 votedFor，接着它恢复了，又收到来自另一个节点的投票请求当前 votedFor 是空，接着给这个节点投了票。这个时候两个节点都认为自己收到了 majority 的投票请求，这样就同时有两个节点成为 leader。

**Volatile State: commitIndex lastApplied nextIndex matchIndex**

Leader 可以通过自己本地日志和往 follower 发送 appendEntries 来判断出当前 commitIndex 和 lastApplied nextIndex matchIndex，follower 在收到 leader 的 appendEntries 之后可以通过 leader 的 commitIndex 和 本地日志来计算出自己的 commitIndex。这样导致问题是 leader 和 follower 有可能重复 apply 数据。

重现日志的过程会很慢，所以 raft 引入了 snapshot


最后贴个图 
<img src="/images/mit6.824.png">
