---
layout: post
title: "MIT 6.824 Lab3A 笔记"
---
### 题目描述

基于 lab2 raft 实现容错 KV DB 集群。key-value service 需要支持少部分节点挂掉或者网络出现分区，但是大多数节点可用的情况下仍能处理客户端的请求。支持 PUT APPEND GET 三种操作。

### Linearizability

系统满足 CP， 强一致性。Consistency 又可以说是 Linearizability。在分布式系统有多个数据副本的前提下，多个 Clients 并发对系统的读写操作，对 clients 输出的结果和只有单个副本单个 server 输出的结果一致。

### 整体实现流程

<img src="/images/lab3-1.png">

1. Client 往 KV server （leader） 发送 Put/Append/Get 请求，Server 收到请求之后，如果改请求是 Put/Append 请求，则判断其有无执行过，如果执行过直接返回成功。
2. Server 将请求通过下层 raft 模块提供的 Start 函数将 Command 发送给 raft。 该 Command 如果在大多数节点执行成功。raft 通过 applyCh 将 applyMsg 发送给 KV server
3. KV server 监听 applyCh，如果有消息将对应的消息发送给响应的 channel。

### 代码实现

#### Client 端

	func (ck *Clerk) PutAppend(key string, value string, op string) {
		Uid := nrand()
		for {
			if ck.leader < 0 {
				ck.leader = ck.randomServer()
			}
			args := PutAppendArgs {
				Key: key,
				Value: value,
				Op: op,
				Uid: Uid,
				ClientId: ck.clientId,
			}
			reply := PutAppendReply{}
			ok := ck.servers[ck.leader].Call("KVServer.PutAppend", &args, &reply)
			if ok {
				if reply.Err == "" {
					return
				}
			}
		ck.leader = -1 }
	} 

1. 客户端随机往 server 发送请求，如果成功则记住当前 leader 的 ID，这样可以节省每次请求需要重新寻找 leader 的时间
2. 客户端每次请求有可能会失败，如果失败则发起重试，直至 server 端返回成功。
3. server 端返回失败有可能是：a.执行失败；b.执行成功但是返回失败。如果 server 端执行成功但是返回失败，如果重试就会导致重复执行，所以每次请求都用一个相同的 UID，Server 端在当前 Command 成功 Apply 之后将此操作记录下来，如果下次请求再有相同的 UID 发送过来则不再重复执行，直接返回成功。


#### 一个错误 server 端的实现

最初我实现的版本用 Condition variable，Server 端每次收到 Command，记录当前的 index，然后等待 KV server 收到的 applyIndex 大于当前的 Index，当前命令则返回成功。起初我认为只要比自己 Index 大的 Command 能执行成功那么当前 Command 一定能执行成功，如果当前 leader 挂掉，请求终止，客户端还是会重试，不影响执行。但是这样写在发生网络分区的时候就会有问题。

一共有 5 台 server。3 是 leader 正常接收请求。假设发生了网络分区：0 1 2 和 3 4。 此时往第二个分区写入数据，返回 Index = 2，接着网络分区恢复，Index = 2 的日志被其他日志覆盖，applyIndex > 2, 这个 index = 2 的日志 kv 3 返回成功。这样实际上 command 没有执行成功，但是 leader 返回了成功。

<img src="/images/lab3-2.png">

#### server 端实现代码

处理来自 raft 的 applyMsg

	func (kv *KVServer) handleApplyMsg() {
		for {
			applyMsg := <- kv.applyCh
			kv.mu.Lock()
			kv.applyMsg = applyMsg
			applyOp, ok := applyMsg.Command.(Op)
			kv.mu.Unlock()
			_, done := kv.latestRequestInfo[applyOp.Uid]
			if done {
				continue
			}
			kv.mu.Lock()
			if ok {
				if applyOp.Op == "Put" {
					kv.KvData[applyOp.Key] = applyOp.Value
				} else if applyOp.Op == "Append" {
					element, ok := kv.KvData[applyOp.Key]
					if ok {
						kv.KvData[applyOp.Key] = element + applyOp.Value
					} else {
						kv.KvData[applyOp.Key] = applyOp.Value
					}
				}
			}
			kv.latestRequestInfo[applyOp.Uid] = true
			kv.mu.Unlock()
			ch := kv.getApplyChByUid(applyOp.Uid)
			close(ch)
			time.Sleep(5 * time.Millisecond)
		}
	}

KV Server 维护请求 UID 到 Channel 的映射。

	func (kv *KVServer) PutAppend(args *PutAppendArgs, reply *PutAppendReply) {
		kv.mu.Lock()
		_, ok := kv.latestRequestInfo[args.Uid]
		kv.mu.Unlock()
		if ok {
			reply.Err = ""
			return
		}
		op  := Op {
			Key: args.Key,
			Value: args.Value,
			Op: args.Op,
			ClientId: args.ClientId,
			Uid: args.Uid,
		}
		index, _, isLeader := kv.rf.Start(op)
		if !isLeader {
			reply.Err = ErrorMsgNotLeader
			return
		}
		ch := kv.getApplyChByUid(op.Uid)
		reply.In = index
		select {
			case <-ch:
				reply.Err = ""
			case <- time.After(500 * time.Millisecond):
				reply.Err = ErrorMsgTimeout
		}
		return
	}
	
Server 端收到请求之后新建一个当前请求的 channel，调用 Start 之后, select 等待该 channel 是否 close，如果 close 返回给客户端执行成功，超时则返回失败。

	func (kv *KVServer) Get(args *GetArgs, reply *GetReply) {
		op := Op {
			Key: args.Key,
			Value: "",
			Op: "Get",
			ClientId: args.ClientId,
			Uid: args.Uid,
		}
		_, _, isLeader := kv.rf.Start(op)
		if !isLeader {
			reply.Err = ErrorMsgNotLeader
			return
		}
		ch := kv.getApplyChByUid(op.Uid)
		select {
			case <- ch:
				kv.mu.Lock()
				element, ok := kv.KvData[args.Key]
				reply.Value = ""
				if ok {
					reply.Value = element
				}
				kv.mu.Unlock()
			case <-time.After(500 * time.Millisecond):
				reply.Err = ErrorMsgTimeout
		}
		return
	}
	
>A kvserver should not complete a Get() RPC if it is not part of a majority (so that it does not serve stale data). A simple solution is to enter every Get() (as well as each Put() and Append()) in the Raft log. You don't have to implement the optimization for read-only operations that is described in Section 8.
	
GET 操作。最初我疑惑为什么 GET 操作也需要从 leader 读取数据并且也要多个节点达成一致才能读呢，不就是一个简单的读取吗？直接从 leader 读就好了。KV server 要求强一致性。如果 read 操作只是判断当前是不是 leader 就读取数据，有可能当前 leader 已经不是 leader，但是它自己不知道（网络出现分区），这样返回给客户端的数据就是 stale data。所以 read 操作也需要多个节点达成一致才能读取。

<img src="/images/lab3A.png">