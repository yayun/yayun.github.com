---
layout: post
title: "MIT 6.824 Lab3B 笔记"
---


3B case 目前还没有完全过，先把整体流程整理下，有时间再 debug。

### Log compaction

raft 日志随着接受的客户端请求越来越大，但是在真实的系统中，日志不能无限增大。随着日志增大，需要更多的内存来存储这些数据，同时 State Machine 根据日志来重现数据也需要更多的时间。

所以引入了 SnapShot。

<img src="/images/figure12.png">

各个 server 的 snapshot 的独立的。一旦日志长度达到限制，就将stateMachine 里的数据持久化，同时需要记录此次 snapshot 最后一条日志的 index 和 term。所以 snapShot 的数据包含 lastIncludedIndex + lastIncluedTerm + kvData。SnapShot 本质上是用当前 stateMachine 里的数据状态替代 raft 的日志，SnapShot 的数据一定是已经 apply 的数据。While having a leader helps avoid
conflicting decisions in reaching consensus, consensus
has already been reached when snapshotting, so no decisions conflict.

### 整体流程

<img src="/images/snapshot1.png">

1. StateMachine applyCh，等待raft 的数据
2. 处理完成之后获取RaftStateSize，如果发现数据大小超过限制，则调用 raft 模块发起 snapShot。传给 raft 当前 stateMachine 中的 kvData，最后一条日志的 index 和 term。
3. raft 发起 snapShot。完成之后删除 logEntries 中 index <= lastIncluedIndex 的数据。


尽管每个 server 都独立 snapShot，如果 follower 的日志落后 leader 太多，leader 也需要将 snapShot 的日志发送给 follower：

1. follower 日志更新的比较慢，leader 已经完成 snapShot，但是 follower 未同步的日志已经被 leader snapShot了。
2. 发生网络分区，minority 节点的分区里面的 server 重新加入
3. 集群中新增加一个节点。

<img src="/images/snapshot2.png">

1. Leader 不断发送心跳给其他节点，如果 rf.lastIncludedIndex >= nextIndex，则需要将 appendEntriesRpc 切换成InstallSnapShotRpc
2. 节点收到 InstallSnapShotRpc 处理请求。
3. 通过 applyCh 通知上层 stateMachine 更新 KvData。这些数据就可以不以 command 的形式 apply 到 stateMachine，直接更改 kvData。

<img src="/images/figure13.png">

### 具体实现

snapShot 之后日志的 Index 不是 logEntries 数组中每个节点的 Index + 1 而是在原来的基础上加上 lastIncluedIndex，所以要修改之前所有涉及到日志的地方。

AppendEntriesHandler。 `args.PrevLogIndex == 0 || (args.PrevLogIndex == rf.lastIncludedIndex && args.PrevLogTerm == rf.lastIncludeTerm)` 这两种情况需要将 follower 自己的日志完全由 leader 的日志覆盖。

InstallSnapShotRpc

	func (rf *Raft) installSnapshotRpc(server int, currentTerm int, lastIncludedIndex int, lastIncludedTerm int) {
		args := &InstallSnapshotRpcArgs{
			Term: currentTerm,
			LeaderId: server,
			LastIncludedIndex: lastIncludedIndex,
			LastIncludedTerm: lastIncludedTerm,
			Data: rf.Persister.ReadSnapshot(),
		}
		reply := &InstallSnapshotRpcReply{}
		peer := rf.peers[server]
		ok := peer.Call("Raft.InstallSnapshotRpcHandler", args, reply)
		if !ok {
			return
		}
		rf.convertToFollowerOrNot(reply.Term, server)
		rf.mu.Lock()
		currentRole := rf.currentRole
		rf.mu.Unlock()
		if currentRole != Leader {
			return
		}
		rf.updateNextIndexAndMatchIndex(server, args.LastIncludedIndex)
	}
	
InstallSnapShotRpcHandler

	// InstallSnapshotRpc handler
	func (rf *Raft) InstallSnapshotRpcHandler(args *InstallSnapshotRpcArgs, reply *InstallSnapshotRpcReply) {
		rf.convertToFollowerOrNot(args.Term, args.LeaderId)
		rf.mu.Lock()
		reply.Term = rf.currentTerm
		lastIncludedIndex := rf.lastIncludedIndex
		rf.mu.Unlock()
		if args.Term < reply.Term {
			return
		}
		rf.resetElectionTimer()
		// If existing log entry has same index and term as snapshot’s
		// last included entry, retain log entries following it and reply
		// 此时 follower 自己没有 snapshot 数据 do nothing and return
		rf.mu.Lock()
		log := rf.getLogByIndex(args.LastIncludedIndex)
		if log.Term == rf.lastIncludeTerm {
			rf.mu.Unlock()
			return
		} else {
			// Discard the entire log
			rf.logEntries = []logEntry{}
		}
		rf.lastIncludedIndex = args.LastIncludedIndex
		rf.lastIncludeTerm = args.LastIncludedTerm
		rf.commitIndex = int(math.Max(float64(args.LastIncludedIndex), float64(rf.commitIndex)))
		rf.lastApplied = 	int(math.Max(float64(args.LastIncludedIndex), float64(rf.lastApplied)))
		rf.mu.Unlock()
		rf.persistStateAndSnapshot(args.Data)
	/	/ Reset state machine using snapshot contents (and load snapshot’s cluster configuration)
		rf.applyCh <- ApplyMsg{
			CommandValid: false,
			CommandIndex: -1,
			Command: args.Data,
		}
	}
	
server 端在启动时需要先从 persist 中恢复数据

监听来自 raft 的消息，applyCh 中收到的 Msg 如果 commandValid == false 则：

    if !applyMsg.CommandValid {
			data, ok := applyMsg.Command.([]byte)
			if ok {
				r := bytes.NewBuffer(data)
				d := labgob.NewDecoder(r)
				if d.Decode(&kv.KvData) != nil {
					DPrintf("decode kvData error \n")
				}
			}
			continue
		}
