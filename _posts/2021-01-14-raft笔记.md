---
layout: post
title: "raft笔记"
---

### Raft 基本概念

Raft 名字源自 Reliable Replicated Redundant And Fault-Tolerant。

Raft 是管理日志复制的分布式共识算法。什么是共识呢？共识问题是分布式计算中最重要也是最基本的问题之一。简单来说，共识的目标是让几个节点就某个事情达成一致，一旦这几个节点就某个事情达成一致，结果就是不可变的。如果集群中的大多数节点能正常工作，raft 算法就能正常工作。假设集群中有 2F + 1 台机器，集群最多可以容忍 F 台机器宕机，如果有大于 F 台机器宕机，集群是不能正常工作的（但是不会返回错误的结果）。

Raft 以 lib 库的形式链接到某些服务器程序中。假设你有一个复制服务，每一个副本都会有 rpc 代码再加上 raft 库。 Raft 库相互协作来确保复制服务的可靠性。

接下来会从三个方面来讨论 raft 算法：

1. leader 选举
2. 日志复制
3. 安全约束

### Leader 选举

通常情况下，raft 集群中的每个节点有三种状态：leader，follower，or candidate。
正常情况下只有一个 leader 其他节点都是 follower。Follower 不会发出 rpc 请求，只会响应 leader 或者 candidate 的请求（appendEntriesRpc  or requestVoteRpc）。Leader 响应所有来自客户端的请求，如果 follower 收到来自 client 的请求它也会把请求转发给 leader。

<img src="/images/figure5.png">

如上图，Raft 将时间分割成一个个任期，任期时间是任意的。任期开始于一个新的选举，term =  election+ normal operaion。每一个任期内至多有一个 leader。T3 没有 leader 说明这次选举没有选出 leader。什么情况下选不出 leader 呢？假设一个集群中有 3 个节点，集群所有的节点的初始状态都是 follower，他们的 election timer 同时到期，（没有收到来自 leader 的 心跳）他们同时发起选举：

1. term + 1
2. 当前的身份从 follower 切换成 candidate
3. 给自己投一票
4. 同时给其他节点发送 requestVote RPC 请求
5. reset election timer

但是由于所有的节点都给自己投了一票，（同一个 term 内只能给一个节点投票）所以不会有节点再给其他的节点投票，所以这个 term 内不会选出 leader。如果集群中所有节点的选举超时时间都相同，那么这三个节点又会在同一时刻内 election timeout，发起新的选举，还是选不出新的 leader。所以 raft 引入了随机的选举超时时间（randomized election timeout）来解决这个问题。选举超时时间从一个给定的区间内随机选取（eg 150 ～ 300 ms）。上述例子中，3个节点的选举超时时间不一致，某一个时间点只会有一个节点超时，就不会出现一直瓜分选票选不出 leader 的问题。

<img src="/images/figure4.png">

Raft 使用心跳机制来触发选举。当服务刚启动，所有的节点都是 follower，当某一个 follower election timeout，触发选举成为 leader 之后，会周期性的往其他的 followers 发送 AppendEntries Rpc（所以心跳时间小远小于选举超时时间才能一直压制其他 follower 发起选举）。如果 follower 一段时间内（election timeout）没有收到来自 leader 的 appendEntries RPC，follower 会认为 leader 不可用，开启选举。当前 candidate 会遇到三种情况：

1. 赢得选举成为 leader 给其他节点发送 appendEntries RPC 告知其他节点我是当前 term 的 leader
2. 其他的节点赢得选举，给当前节点发送 appendEntries RPC，如果这个 leader 的 term 比自己大，当前 candidate 退回成 follower 状态。但是如果这个 leader 的 term 比自己小，则返回失败，仍维持 candidate 状态。
3. 一段时间过去（election timeout）仍然没有选出 leader， 开启下一轮选举

当前 candidate 如果在当前 term 内赢得大多数节点的投票，它就赢得了这次选举。其中每个节点在某个 term 下只能给一个节点投票。


什么情况下一个节点会给另一个节点投票

1. candidate 发来的 term 下没有给任何人投过票
2. candidate 的 term >= currentTerm
3. candidate 的日志 at least as up-to-date as current‘s log


### 日志复制

一旦 leader 被选中，则开始处理来自客户端的请求。正常情况下，leader 收到来自客户端的请求

1. 将日志写入本地日志
2. 同时往其他节点发送 appendEntries 让其他节点复制该条日志
3. 如果该条日志在大多数节点上执行成功，leader 将该条日志视为 committed，并将该条日志 apply 到本地状态机（state machine），同时返回给客户端该条指令执行成功。follower 如果发现该条日志是 committed 状态，也会将该条命令 apply 到本地状态机。
4. leader 发送 appendEntries，follower 有可能执行失败，此时 leader 会不断重试直至所有的 follower 都执行成功。

日志格式：
<img src="/images/log.png">

每一个方格代表一条日志，每条日志包含任期号和要执行的命令。

正常情况下 leader 和 follower 日志保持一致，appendEntries rpc 不会返回异常，所有节点的日志可以保持一致。但是如果 leader 崩溃就会导致多个节点的日志不一致。

<img src="/images/leader-crash-log.png">

上图是可能出现的日志不一致的情况。	a-b：follower 缺失部分日志；c-d：follower 比 leader 多出了一些日志；e-f：也有缺失日志也有可能多出日志。

Raft 通过强制让 follower 复制自己的日志来保证日志的一致。Leader 给 follower 发送 appendEntries 携带几个参数：term， leaderId， prevLogIndex， prevLogTerm， entries[]， leaderCommit。其中 prevLogIndex 代表上一条日志的索引，prevLogTerm 代表上一条日志的任期号。follower 收到请求后会根据 prevLogIndex 找日志

1. 找到 prevLogIndex 对应的日志，但是 term 和 prevLogTerm 冲突，则删除 prevLogIndex 和 之后的所有日志。返回失败。
2. 找不到 prevLogIndex 对应的日志，返回失败
3. 如果能找到则匹配 prevLogTerm prevLogIndex 的日志，则将 leader 发来的日志 append 到自己的日志上面（这里有一个细节问题，后面写 mit6.824 的时候会详细说明）

Leader 为每一个 follower 都维护一个 nextIndex，代表 leader 将要给 follower 发送的日志索引，如果 appendEntries 返回失败，leader 会让 nextIndex -1，直到日志匹配成功。

### 安全性约束

旧的 leader 崩溃之后什么样的节点能被选举成为新的 leader？如果仅仅按照谁的 term 大谁就可以当选为 leader，日志不全的节点当选为 leader 会导致已经 commit 的日志被覆盖，每个节点的本地状态机的日志就有可能不一致。

**选举限制**

Candidate 发送选举请求给其他节点之后只有满足这两个条件其他节点才会给当前 Candidate 投票：

1. candidate 发来的 term >= currentTerm
2. 投票人在 candidate 发来的 term 下没有给任何节点投过票（包括自己）
3. candidate 的日志至少要和当前节点的日志一样新。

Raft 通过比较最后一条日志的 index 和 term 来判断两个节点的日志是否一样新？

1.  candidate 最后一条日志的 term 大于投票人最后一条日志的 term
2. 最后一条日志的 term 相同，candidate 最后一条日志的 index 大于等于 投票人的最后一条日志的 index

**处理前任 leader 没有来得及 commit 的日志**

<img src="/images/log-replica-bad-case.png">

 1. a. S1 为 leader，term = 2
 2. b. S1 崩溃，S5 当选为 leader，term=3 （S3 S4 S5 投票）。同时S5 接受了一个新的日志 
 3. S5 崩溃，S1 重启，重新当选为 leader （S1 S2 S3 S4 投票）。重新开始日志复制，将 term=2 的日志复制到了大多数节点上
 4. 如果这个时候 S1 崩溃（d），S5 选举成为 leader （S2 S3 S4），会用 term=3 的日志覆盖其他节点相同索引处的日志。

 term2 的日志已经被大多数节点复制了，日志为 committed 状态但是这种情况下仍然有可能被覆盖。出现这种现象的根本原因是：S1 在 term=4 的时候单独 commit 了 term=2 的日志。为了避免出现这种现象。Raft 不允许 leader 单独提交之前任期的日志，而是在提交当前任期的新日志时候，顺带将旧日志一并提交, 这样就不会出现最后一条日志的 term 很小的情况。根据这个限制 d 就不会出现了（比较最后一条日志的 term S5 就不会当选），而是在 e 中，raft 只提交 term=4（自己任期）的日志顺带将 term=2 的日志提交。具体实现中，可以在 leader 当选之后提交一条 no-op 日志。
### 参考：

1. [https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)
2. [http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/)
3. [https://thesquareplanet.com/blog/students-guide-to-raft/](https://thesquareplanet.com/blog/students-guide-to-raft/)
4. [https://en.wikipedia.org/wiki/Raft_(algorithm)](https://en.wikipedia.org/wiki/Raft_(algorithm))
5. [https://raft.github.io/](https://raft.github.io/)
6. [https://pdos.csail.mit.edu/6.824/schedule.html](https://pdos.csail.mit.edu/6.824/schedule.html)
7. [https://yinzige.com/2019/04/19/raft-notes/](https://yinzige.com/2019/04/19/raft-notes/)


