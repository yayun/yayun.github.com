---
layout: post
title: "GFS 笔记"
---

Why big storage is so hard？

>high performance -> shard data over many servers
>
>many servers -> constant faults
>
>fault tolerance -> replication
>
>replication -> potential inconsistencies
>
>better consistency -> low performance
 
如果数据量很大，我们想要读取/写入的高性能，需要把数据分片，分配到多个机器上存储，但是数据存储在多台机器上就需要考虑到容错。
怎样容错呢？需要将数据复制保存在多个副本上。
但是数据有多个副本就涉及到多个副本的数据一致性问题，我们要保证多副本的数据一致性就需要牺牲性能。

GFS 是什么？

GFS 的全称是 Google File System。它涵盖了分布式系统的所有知识：fault tolerance. parallel performance. replication. consistency。
google 开发出来的，最初为了满足 google 日益快速增长的数据存储需求。

设计这个系统需要考虑的问题：

1. 机器故障很常见：应用程序出问题，操作系统 bug，人为的错误，或者是磁盘，内存，网络故障，机器断电等等... 所以在设计这个系统时需要考虑到持续监控，错误检测，容错，崩溃恢复。
2. 文件很大, 一个文件有几 GB 是常有的事情，因此在设计系统时，需要考虑到 I/O 操作和 操作块大小
3. 大部分文件的修改是追加而不是覆盖，在一个文件内的随机写几乎是不存在的，一旦文件写入，文件通常只会被顺序读取。因此实现时，我们最需要考虑的是文件的追加，保证其原子性

## Design Overview

### Assumption

1. 系统有很多不是很昂贵的机器组成，这些机器有可能会经常宕机。需要考虑故障检测和崩溃恢复
2. 系统有两种读取：大块的顺序读和小块的随机读。
3. 有很多大的顺序 append，一旦写入文件几乎不会被修改。在任意位置文件修改也支持，但是其性能会很差（有可能会阻塞住所有的操作，因为要重新修改 master 所有的 chunk 信息）
4. 需要考虑到多个 client 往同一个文件并发 append 的情况，要保证其原子性

### Interface

create，delete，open，close，read，write，snapshot，record  append operations

### 整体架构

<img src="/images/GFS.png">

一个 master + 多个 chunkserver  + 多个 client
 
一个大文件被分成固定大小的 chunk。chunk 存储在 chunkserver 上面，为了数据的可靠性，默认 chunk 有三个副本存储在三个不同的 chunkserver 上面。

### Master Data

<img src="/images/masterdata.png">

master 保存所有文件的 metadata，包含文件名，访问控制信息，文件到 chunk 的映射，chunk 的当前存储的地址。

在 master 中有两个主要的 table: 一个 table 存储 filename 到 chunkId （each chunk is identified by an immutable and globally unique 64 bit chunk handle assiggned by the master at the time of chunk creation）数组的映射。每个 chunk 相关的信息： 一般每个 chunk 都会有三个存储在 chunkserver 上的副本，master 需要存储每个 chunkId 对应的 chunkserver 的信息；当前 chunk 信息的版本号；当前哪个 chunk 是 primary；还有当前 primary 的有效期。

master 所有的数据都存储在内存中，所以 master 内存大小是不是整个系统能存储 chunk 数量的瓶颈呢？对于每个 64M 的 chunk，master 存储其元信息只需要少于 64bytes 的大小。如果文件数量越来越大，我们也需要给 master 增加额外的内存。

#### chunk size
每个 chunk 的大小是 64 M。

优点：

* 减少 client 和 master 的交互。对同一个 chunk 的操作只需要请求一次 master 即可。
* 对于一个大的 chunk，client 可能会对这个 chunk 做很多额外的操作，通过在 clinet 和 chunkserver 之间保持一个持久的 TCP 连接，可以减少额外的网络开销。
* 可以减少 master 中 metadata 大小

缺点：

小文件会成为热点。解决办法：增加副本数或者错开 client 获取文件的时间。


#### chunk location 

其中 chunk 存储在哪些 chunkserver 上这些数据是可以丢失的，因为只有 chunkserver 自己能决定数据有没有存在自己的磁盘上面，如果把这些信息存在了 master 上面，chunkserver 发生崩溃了之后，数据会丢失，client 再去这个机器上面找数据，数据有可能已经不存在了。另一方面，通过 master 和 chunkserver 之间周期性的通信，master 就可以知道哪些 chunkserver 新加入集群，chunkserver 修改名字或者宕机了，重启了等信息。这样就能实时的维护 chunkId 到 chunkservers 的映射信息。	


#### chunk version number 

chunk version number 仅仅在 master 重新分配 primary 时增加

这里有个问题，master 崩溃恢复会根据 version 指定一个 chunkserver 作为 primary，其他节点是 secondary，但是如果发现 chunkserver 的 version 都大于自己的 version，此时 master 会认为自己在分配 version 中发生错误（有可能先通知 chunkserver version 之后自己才做的 version 持久化），会用这个比较大的 version 作为自己的版本号，并将此 chunkserver 设置为 priamry

4. 如果 master 中 version 大于任意一个 ckserver 的版本号，会再次分配primary
5. version 是为了找到具有最新的 chunk 的 server，这样的 server 有成为 primary 的能力，如果 master 挂掉，只有这些 version 最高的 primary 和 secondary 能处理请求


#### lease

租约， 默认有效期是 60s。

租约可以防止脑裂，如果没有租约，leader 仅仅靠心跳来维护和 primary 之间的通信，但是如果只是由于网络问题，leader 发出的心跳没有得到应答，leader 重新指定了一个新的 primary，但实际上当前 primary 还活着，所以可能会有两个 primary 来处理请求。

* client 会短时间内缓存 primary 的信息，所以新的写请求有可能还是会打到旧的 primary 上面。）
* 还有一种情况是，client 请求 master 告诉其 primary 是谁，接着 master 回复了 client，但是这时 master ping 了 primary，primary 没有回复，如果没有租约，master 会指定一个新的 primary，但是这时旧的 primary 还会处理请求，这样就会出现两个 chunk 数据不一致的问题。

有了租约，如果 master 无法与 primary 心跳续约，master 需要等待租约到期之后再指定一个新的 primary，这样在租约到期之后老的 primary 也不会再处理客户端的请求，租约到期之前可以等待旧的 primary 把所有的 client 请求完。

#### primary

被授予租约的 chunkserver 称为 primary。当 chunk 的数据发生变化时，如 write 和 append 操作，primary 会选定一个修改顺序，修改 chunk 的数据。当前 chunk 其余的副本会根据这个顺序来写入。

primary 可以无限续约，primary 向 master 发请求，master 会通过心跳来通知 primary 重新给其续约，并且通过心跳通知给所有的 replicas(此时 chunk version number 会发生变化) （These extension requests
and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunkservers.）。

同样 master 也可以在某个 lease 过期之前收回。

primary 是易失的，如果当前没有 primary，master 会重新根据 version 信息来授予某个 replica lease。

#### operaion log
由于 metadata 是存在 master 的内存中的，所以 master 还需要存储自己数据变化的日志，一个文件增加了一个 chunk 信息或者新指定一个 primary，或者某个 handle 的版本号发生变化，这些 master 数据发生变化都必须追加一条日志到 log 中。

如果 master 崩溃了，我们需要从日志中重建状态，但是 master 从启动到当前时间可能很长时间，所以 master 需要额外创建一份完整的状态到磁盘。创建 checkpoint 需要花费一些时间，所以 master 为了不妨碍后续日志的修改，在创建 checkpoint 的时候，会先创建一个对当前日志的新的 copy，然后启动一个新的线程对这个 copy 日志做 checkpoint。完成之后会将数据存到本地磁盘和远程备份。（The master switches to a new log file and creates the
new checkpoint in a separate thread）checkpoint 在磁盘中以 B-tree 的方式存储。

当 master 崩溃重启后只需要回到最近 checkpoint 的位置，再重演后面的日志就可以恢复自己的状态。旧的 checkpoint 和 日志文件会维护一段时间之后再删除。

我们可以把 metaData 类比成 redis 存储的数据，operation log 类比成 AOF，checkpoint 类比成 RDB。

## Consistency Model


### Guarantees By GFS

某段文件区域发生数据更改之后，其状态取决于修改的类型，修改是成功还是失败，是并发还是串行

GFS 中数据修改分成两种：普通写入和记录追加。

* 普通的写入是由客户端指定写入的文件 offset 
* record append 是由 GFS primary 指定 offset，原子的至少执行一次。因为如果某一个 chunkserver 执行失败，client 再次追加直至执行成功。

文件状态

* **consistent** 
	
	数据在每个 chunkserver 上都是一致的
	
* **defined**

	客户端在修改完成之后可以看到其修改的结果，并且数据是 consisten 的
 （A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.）

GFS 解决一致性问题：

* 系统中只有一个 primary，其他副本都以 primary 的数据为准，当其他副本错失数据写入时机时，master 通过 version 机制来检测 stale chunk server 进而回收。
* 写入的顺序都是由 primary 来定的，其他的 chunkserver 按照相同的顺序来写入。

有了 consistent 的概念之后为什么引入了 defined 的概念呢？

因为在普通写入的时候（客户端指定 offset），如果客户端写入的数据太大，超过了当前 chunk 能写入的最大的数据，此时 GFS 客户端就会将写入拆分成两个，因为写入的顺序都是由 primary 来定的，所以这两个写请求之间有可能会被其他的并发写入干扰，覆盖了之前的更新，在第二个写请求结束之后，客户端看到的数据和它预想的写入的数据不一样，这样数据就是 undefined，但是由于多个副本之间数据写入的顺序都是 primary 来指定的，只要写入成功数据都是 consistent。（If a write by the application is large or straddles a chunk
boundary, GFS client code breaks it down into multiple
write operations. They all follow the control flow described
above but may be interleaved with and overwritten by concurrent operations from other clients. Therefore, the shared file region may end up containing fragments from different
clients, although the replicas will be identical because the individual operations are completed successfully in the same
order on all replicas. This leaves the file region in consistent
but undefined state。）
	
<img src="/images/gfs-file-region-state-after-mutation.png">

上图展示了几种文件修改操作之后文件的状态

* 普通的写入（serial） ，如果写入成功，文件区域的状态是 defined (这个比较好理解)
* 并发普通的写入，写入成功，数据是 undefined but consistent。是因为写入数据量比较大有可能把写入拆分成两个请求。
* 失败的修改 undefined unconsistent
* record append 写入的数据是 defined 这个比较好理解，因为都是 primary 来指定 offset，每个 chunkserver 都按照 primary 指定的 offset 和顺序来写入，所以数据最终都是 defined。但是为什么会是 defined interspersed with inconsistent 呢？论文中其实也有说明：
In addition, GFS may insert padding or record duplicates in
between. They occupy regions considered to be inconsistent
and are typically dwarfed by the amount of user data. 
padding 和 record duplicates 是怎样产生的呢？
在论文中的 **Atomic Record Appends** 这一节有详细说明，下面会详细说明。

client 会缓存 chunkserver 的地址，所以客户端有可能会读到 stale data，但是大部分情况下我们数据修改的类型都是 append，所以我们可能会读到旧的数据但是不会读到过期的数据。当 client 重新向 master 获取 chunk 信息后还是会读到新的信息。


### Implication for Applications 

实际上，GFS 应用比较普遍的场景是文件的追加而不是文件的随机写。

客户端有两种使用场景，一种场景是只有一个 writer 写文件。

* writer 从头到尾写一个文件，在写完所有的数据之后将文件重命名。这样文件对客户端来说要么完全可见要么完全不可见。

* 周期性的设置 checkpoint，chekpoint 之前的数据被认为是 defined，所以 readers 只需要读取 checkpoint 之前的数据就可以了。如果 writer 故障重启，只需要从上一个 checkpoint 开始增量的写入就可以了。

这两种方式都可以保证 reader 读到的数据都是一致的

还有一种场景是多个 writer 并发的写一个文件。gfs 的 append 操作是 defined interspersed with inconsistent，因为有 padding 和 duplicates record 所以有些区间的数据是不一致的，即相同的 offset 在不同的 chunkserver 读到的数据不一致。为了解决这种情况：

* writer 在写数据的时候，也会把写入数据的校验和存到本地，reader 读入的数据的会将读取的数据 checksum 和本地存储的 check sum 做对比，如果不一致，认为这块数据不是自己写入的而是 gfs padding 的，跳过这块内容读取。
* 对于 duplicate record，writer 在写入数据的时候可以带一个 unique id，读取的时候，如果已经读到过这个 id 的数据则跳过。

上述这些代码都在 gfs 客户端的 library 中实现，应用代码可以很方便的调用。‘
checkpoint may include application-level checksums ？？ 这句话不太明白
客户端的 checksum 到底是怎么做的？

##  System Interactions

### Read

1. client 会根据指定的 chunk 的大小（64MB）将文件名和文件偏移量（客户端只是像读取这个文件指定字节返回的数据）转换为文件中的 chunk index。
2. client 向 master 发送请求，请求参数：chunk index + filename；master 会将相应的 chunk handle 和当前 chunk 的地址返回给客户端。客户端收到响应之后会将 master 返回的数据缓存，key = filename + chunk index。
3. 接着客户端会向 master 返回的其中一个副本（通常是距离 client 最近的那个副本）请求数据。请求包含指定的 chunk handle 和 客户端需要读取的这个 chunk 的字节返回。chunkserver 将数据返回给 client。如果后续客户端的请求还是请求相同的 chunk，就不需要和 master 交互，只需要和 chunkserver 交互，直到缓存数据过期或者重新读取另一个文件。

现实情况中，client 请求的数据可能会跨过多个 chunk，client 再请求第一个 chunk 的信息时，master 也会将随后的 chunk 的信息返回给客户端。这些额外的信息可以避免后续的 client 和 master 的交互。（GFS libray: If the application wants to read more than 64 megabytes or even just two bytes but spanning a chunk boundary. so the applications linked with a libray that sends our rpcs to the various servers and that library would notice that the read spanned a chunk boundary and break it into two separate read and maybe talk to the master. May be that you could talk to master once and get two results or something but logically at least it two request to the master and then request to two different chunk servers）

client 和 chunkserver 都不会缓存文件数据。一般文件都会很大很难缓存，缓存带来的收益不是很大。不缓存数据也消除了缓存一致性带来的问题。chunkserver 不需要使用缓存，因为文件都存在 linux 本地文件系统，Linux 文件系统缓存会讲经常访问的数据放到内存里面。

### write

<img src="/images/gfs-write.png">

1. client 给 master 发送请求，master 需要将当前 primary 和 其他的 replicas 返回给客户端

	* 如果当前没有 primary，master 需要重新指定一个 priamry。 master 会存储 chunk 当前最新的版本号，匹配具有最新版本号的 replica，将其指定为 primary，其他 replica 定位 secondary，接着 master 将版本号递增写入磁盘。假设 replca 崩溃启动，最新版本号的 replica 在几分钟之内没有启动成功，master 会通知客户端等待一段时间，等待最新版本号的机器启动成功。
	* 如果 master 重启之后发现某个 replica 的版本号高于自己的版本号，master 会假设自己在分配版本号是发生了错误，会用当前 replica 的版本号作为当前 trunk 的版本号，并且用当前 replica 作为 primary。个人认为 master 在更新版本号的时候是先通知 replica 再写入本地磁盘。这样可以避免更新完自己之后崩溃，恢复后发现没有 replica 的 version 比自己高从而选不出 primary 的情况。

2. master 将当前 trunk 的 primary 和 secondary 返回给客户端。**客户端缓存这些信息，当 primary 的 lease 过期或者 primary 宕机的时候，客户端会重新请求 master 获取新的 primary 信息**。
3. 客户端将需要写入的数据按照一定的顺序发送给 primary 和 secondaries。所有的 chunkserver 将数据存在本地的 LRU buffer cache 中，直至数据被使用或者过期。
4. 一旦确认所有的 chunkserver 都接收到了数据，client 会发送给 primary 一个写请求。primary 给所有的客户端发给 mutation 都做了唯一编号。primary 将这些 mutation 以指定的顺序写入本地
5. primary 将写请求转发给所有的副本，每个副本都以 primary 指定的顺序将 mutation apply 到自己本地。
6. secondary 如果完成写入将自己写入成功的消息告诉 primary
7. 最后 primary 告诉 client 此次写入成功 or 失败。如果执行失败，client 会重试直至其执行成功。

这里有个问题，就是如果写入数据很大超过了当前 chunk 了，client library 会将 mutation 请求拆成多个，如果这多个写请求之间还有其他的写请求插入进来的话，那么这次写入就是 undefined。但由于写入的顺序都是由 primary 来指定的，所以最终 file region 是 consistent。

(If it had failed at the primary, it would not
have been assigned a serial number and forwarded.)？？ 这块没有看懂，是先分配编号再写入还是先写入成功再分配编号？？？

### Data Flow

将数据流和控制流解耦主要是为了高效的利用网络带宽。控制流是从 master 到 client 再由 client 到所有的 secondaries，数据是以流水线的方式在各个 chunkserver 之间线性传递。

* 为了充分利用每台机器的网络带宽，避免网络瓶颈和高延迟连接。每台机器在收到数据之后将数据发送给离它最近的机器，再由接收到的机器再转发给距其最近的机器。可以通过 IP 地址来判断节点网络上的远近关系。
* 以流水线的方式来传输数据。一旦 chunckserver 收到数据就会将数据转发。将 B 字节的数据传输到 R 个副本， 理想运行时间是 B/T + RL，其中T是网络吞吐量，L是在两台机器之间传输字节的延迟。传输速率是 100mbps (T)， L 远低于1ms，因此传输 1MB 的数据理想情况下在 80ms

### Atomic Record Append

record append 是原子的。

普通的写入因为是客户端指定的 offset，在写入数据量很大需要将写入拆分成多个请求的时候，写入的文件区域有可能会有其他的写入，所以不是原子的。如果传统的写入也想是原子的，客户端可以用分布式锁来实现。

record append 客户端只需要发送数据，写入的 offset 由 primary 来指定所以可以保证是原子的。写入的流程和上面我们将的写入流程一致，有一点不同：

* primary 在收到数据后会检查当前写入是否会导致当前 chunk 超过限制（64MB），如果会，priamry 会 pads 当前 chunk 到 maximum size (64MB), 同时会通知其他的 chunkserver 做相同的事情，同时会告诉客户端当前数据超过最大限制，需要在下个 chunk 重试（所以 record append 数据大小被限制在 maximum max size 的 1/4）
* 如果 record append 失败，client 就会重试当前操作。record append 只要在一个 chunkserver 执行失败都会告诉客户端重试。所以会导致有些数据重复。GFS 不保证数据在各个副本上完全相同（bytewise identical）但是保证数据作为一个原子单位至少写入一次。

因此 record append 只要写入成功，就会在所有副本上的相同的 chunk 上相同的 offset 存在。因为上述由于写入太大 GFS 进行的 padding 和写入失败导致的 duplicate record，所以我们认为 record append file region 是 defined interspersed with
inconsistent。上面 **Implication for Applications**这一节中也有说明怎样才能防止客户端读到这样的数据。

### SnapShot

快照会瞬间建立一个当前文件或者目录树的拷贝，最大程度的减少对当前正在进行的 mutation 的中断。使用 GFS 时，我们可能想创建一个当前数据的 copy，后续的修改可能会被 commit 也可能会被 rollback。

和 AFS 一样，使用写时复制（Copy-On-Write）来实现 snapshot。

* 当 master 收到 snapshot 请求后，master 会撤回 snapshot 所涉及的所有 chunk 的 lease。之所以撤销 lease 是为了让后续在这些 chunk 上的写请求操作都需要和 master 交互。可以让 master 有机会来创建这些 chunk 的新的 copy。
* 当这些 chunk 的lease 过期或者被撤回之后，master 将所有的操作记录日志到磁盘。 接着 master 将这些文件涉及的元信息进行拷贝，拷贝的元信息逻辑上是新文件，实际上其数据仍指向原文件。

之后如果有客户端想对其中某个 chunk C 写入数据，由于这个 chunk 没有 primary，client 向 master 发送请求，master 会发现 chunk C 的 reference count 大于 1 （因为原 metadata 和 snapshot 的 metadata 都指向 chunk C）master 会先延迟回复这个 client 的请求，先创建一个新的 chunk handler C‘，接着通知所有的 chunkserver chunk C 需要新创建一个副本 C‘，所有的 chunkserver 都会在本地创建一个 C 的副本 C’。这样后续的操作和普通的写入请求没有差别：master 会在 C' 的多个 chunkserver 中选一个当 primary 然后告诉 client 哪个 chunkserver 是 primary。


## Master operation

**The master executes all namespace operations. In addition, it manages chunk replicas throughout the system: it
makes placement decisions, creates new chunks and hence
replicas, and coordinates various system-wide activities to
keep chunks fully replicated, to balance load across all the
chunkservers, and to reclaim unused storage.**

### Namespace Management and Locking

master 有些操作比较耗时, snapshot 会撤销所有涉及到的 chunk 的 lease。但是此时我们不想阻塞其他 master 正在执行的操作，因此我们在相应的命名空间的区域内加锁让需要操作此区域的其他操作等待，同时不涉及此区域的操作可以并行执行。

不像其他传统的文件系统，GFS 没有针对列出每个目录下所有文件的数据结构，也没有针对某个文件或者目录取别名的操作。GFS 将其命名空间表示为一个完整的路径名到 metadata 的查找表。使用了**前缀压缩**，可以让这个查找表高效的在内存中存储。

这个命名空间树（namespace tree）的每个节点，无论是一个文件名或者是一个目录名，都有一个相应的读写锁。在 master 对其操作时都需要先获取对应的锁。如果某个 master 的操作涉及了 /d1/d2/.../dn/leaf ，那么 master 需要首先获取 /d1, /d1/d2, ...,
/d1/d2/.../dn 这些目录的读锁，然后根据操作类型获取 /d1/d2/.../dn/leaf 这个路径的读锁或者写锁，leaf 有可能是一个目录也有可能是一个文件名。

举例来说，假设我们创建了 /home/user 的一个快照 /save/user, 此时 snapshot 操作获取了 /home、/save 两个目录的读锁和 /home/user、/save/user 的写锁。假设此时 master 想创建 /home/user/foo，需要获取 /home、/home/user 的读锁和 /home/user/foo 的写锁，因为 /home/user 的写锁冲突（/home/user 的写锁被持有，别人不可以读取），所以此操作必须要等待 snapshot 操作释放了锁才可以进行。GFS 没有真正的文件系统的层级的组织或者类似 innode 的数据结构需要被保护，所以创建文件不需要此文件所在的父级目录的写锁。在名称上的写锁足以保护父级目录不被删除。

使用这种锁机制的一个很好的特性是，它允许在相同目录下的并发修改。eg: 在相同的目录下，多个客户端可以并发的创建多个文件，只需要每个获取同一个目录下的读锁防止这个目录被删除、重命名或者 snapshot。文件名上的写锁会将两次尝试创建同名文件的操作串行化。

每个操作都获取锁，这样会不会造成死锁呢？

命名空间可以有很多的节点，读写锁对象是惰性分配的，一旦不再使用就会立即删除。此外，为防止死锁，锁的获取顺序是一致：不同层是按照命名空间树从上到下，同一层是按照字母序的顺序。


### Replica Placement

GFS 集群高度分布体现在多个层次上。通常有数百个 chunkserver 分布在多个机器机架上，这些 chunkserver 会被来自自己机架上或者不同的机架上的 client 访问。不同机架上的的两台机器之间的通信通过一个或者多个网络交换机。机架内的的网络带宽一般会大于机架间的网络带宽。多级分布为数据的可伸展性、可靠性和可用性提供了独特的挑战。

因此某个 chunk 的 replica 的放置位置需要遵循两个目的：

* 最大化数据的可靠性和可用性
* 最大化网络带宽的利用率

考虑到这两种情况，仅仅将多个副本放置在不同的机器上是不够的，这仅能应对某台机器出故障，最大化利用每台机器的网络带宽。我们还需要将副本放置在不同的机架上，这样可以容忍整个机架出故障。这样意味着对同一个 chunk 跨机架的读取可以充分利用多个机架上的网络带宽，但是写可能需要跨域多个机架，这也是我们甘愿作出的取舍。


### Creation,Re-replication,Rebalancing

某个 chunk 的副本被创建有三种情况：

* chunk 初始被创建
* 再复制
* 再平衡 

master 为一个新创建的 chunk 选择位置所考虑的几点因素

* 放置副本的节点的磁盘利用率比较低，这样随着时间推移可以让多个 chunkservers 磁盘利用率趋于均衡
* 限制每个 chunkserver 最近新建 chunk 的数量。GFS 的场景一般是单次写入紧跟多次读，为了防止连续的写入带来的负载，尽量将连续的写入分散到多个节点。
* 将副本放置在多个机架上

某个 chunk 的 replica 的数量少于用户设置的值时，master 会 re-replicates 这个 chunk。发生副本再复制的原因有很多：机器宕机、机器磁盘故障、复制目标增加。当有多个 chunk 需要备份时，master 会根据以下三个因素来决定哪个 chunk 优先复制：

* 优先选择可用 replica 少的进行复制
* 根据文件的活跃状态，优先选择文件活跃状态多的 chunk，对于最近刚刚有删除操作的 chunk 延迟复制。
* 优先选择对正在运行的 application 影响较大的复制操作，有些 replica 不足有可能会阻塞客户端，所以优先选择这种操作

GFS 会根据上面三个因素加权排序，选择排序最高的 chunk 优先复制。新的 replica 放置位置遵循的准则和 replica 创建时遵循的一致。
为了防止 clone 操作影响客户端的情况，GFS 限制了每个集群和单台 chunkserver 进行 clone 操作的进程数。此外，通过限制对每台 chunkserver 读请求的数量来减少每台 chunkserver 花在 clone 操作的带宽。

Master 会周期性的 re-balance replicas：它会检测当前 replica 的分配情况，为了更高的磁盘空间利用率和负载均衡来移动 replica 的位置。对于一个新的 chunkserver，master 会逐渐填满其空间而不是立即用接踵而来的 write 操作和新建的 chunk 来填满它。此外 master 还会选择删除一些 replica，通常会选择移除一些磁盘剩余空间低于平均值的一些 chunkserver，以此来平均磁盘空间使用率。

### Garbage Collection

一旦一个文件被删除，GFS 不会立即回收其物理空间。通过一个定期垃圾回收机制来回收这些文件级别或者 chunk 级别的物理存储空间。

#### Mechanism

如果文件被 application 删除，master 会将这个操作记录到日志。但是 master 不会立即删除这个文件而是将其重命名，文件名包含删除时间。master 会定期对文件系统命名空间进行常规扫描，如果这种文件存在时间大于 3 天（可以自定义），master 就会将其删除，同时删除对应的 metadata。这样可以有效的切断其与所有 chunkservers 的连接。在删除之前，文件仍可以通过新指定的名字访问，可以通过重命名恢复。

在 master 和 chunkservers 之间常规的 HeartBeat，chunkserver 会上报给 master 它当前持有哪些 chunks，master 会告诉 chunkserver 有哪些 chunk 已经在它的 metadata 中不存在了。这样 chunkserver 就会删除这些 chunk 信息。


#### Discussion

尽管对编程语言来说垃圾回收是一个复杂的问题，但是在我们的 case 中非常简单。文件名到 chunks 的对应关系都由 master 来维护。而所有 chunks 都在每个 chunkserver 上（Linux 文件系统），因此在 chunkserver 上存在但是不为 master 所知的 chunk 都是 "garbage".

此种形式的垃圾回收方式（异步删除）相较与立即删除由诸多优点：

* 某个 chunk 创建时，在某些 chunkserver 成功，在另一些 chunkserver 上可能会失败（整个 creation 没有成功），这样会遗留一些 chunk 数据，master 没有记录其存在。 同时如果 master 想要删除一些副本，如果同步发消息删除，删除消息有可能丢失，master 需要记录哪些操作失败并重试直至操作成功。在 GFS 设计体系中，所有的有效的 chunk 数据都由 master 记录，master 周期性的和 chunkserver 通信来告诉 chunkserver 哪些 chunk 在这里没有记录需要 chunkserver 来删除。有效的避免了上述问题。
* 删除的时机异步的处理，master 和 chunkserver 都可以选择在系统负载比较低的情况下删除可以让他们能更好的响应客户端的请求。
* 异步删除相比同步立即删除可以避免误删操作

在我们磁盘空间比较紧张的时候，异步删除也有缺点。为了解决这种情况 GFS 做了以下优化：

* 如果已经删除的文件（重命名）客户端再次通知 master 做删除操作，master 会加快其删除的进度。
* 用户可以自定义一些复制和存储空间回收策略，eg： 用户可以指定某些目录下的文件不需要做多个备份；某些文件不需要惰性删除，如果删除可以立即删除。

###  Stale Replica Detection

如果某个 chunkserver 宕机，其有可能会丢失一些修改，这会导致该 chunkserver 上存储的数据过期。为了防止这种情况，master 为每个 chunk 维护了 chunk version number 用以区分最新的和过期的 replica。

当 master 为某个 chunk 所在的 chunkserver 授予一个新的租约时，会增加这个 chunk 的 version number 并且通知所有的 replica。在旧的 version 过期，新的 version 还没有更新到所有的 replica 之前，当前 chunk 的所有的 replica 不会写入新的内容。如果在更新 version 的时候，某个 replica 恰巧不可用，那么这个 replica 的 version 就不会被更新。当宕机的 chunkserver 重启之后会向 master 报告其上所有 chunk 的 version 的时，master 会知道当前 replica 的数据是过期的。另外在 master 和 chunkserver 通信时，如果 master 发现某个 chunkserver 的某个 chunk 的 version 高于自己的，master 会认为在分配 version 时发生了某些错误，会用高的 version 来更新自己。

master 会在定时的垃圾回收任务中清除这些过期的 replica。在回收之前，如果 client 向 master 请求 chunkserver 信息，master 不会将 stale replica 返回给客户端。另外，master 在返回给 client 最新的 primary 信息或者指导 chunkserver 向另一个 chunkserver 复制数据时，还会将当前 chunk 最新的 version 返回给对方，client 或者 chunkserver 会检查它请求的 chunkserver 的 version 是否是最新的数据。

##  FAULT TOLERANCE AND DIAGNOSIS

### High Availability

怎样做到高可用：fast recovery and replication （故障快速恢复和高可用）

#### Fast Recovery

master 和 chunkserver 都被设计成无论何总原因的崩溃都可以在几秒钟重启恢复其状态。客户端和其他的 server 会有短暂的中断，然后重连，重试请求。

####  Chunk Replication

每个 chunk 会复制在多个机架上的多个 chunkserver 上。用户可以对不同命名空间下的文件的副本数做调整，默认是 3 个副本。同时为了满足日益增长的只读存储需求，GFS 也在探索其他形式的跨服务器冗余的方案，例如：parity or erasure codes（EC 将数据分片存储，再多存储一份冗余，如果分片有数据丢失，可以通过冗余来找回）。GFS 希望在当前松耦合的系统下实现这些更复杂的冗余存储模式是有挑战的但是是可管理的，毕竟系统的流量主要是追加写和顺序读而不是随机写入。

#### Master Replication

master 所有的数据也是有备份的。操作日志和 checkpoint 被存储在多台机器上。master state 的一个改动仅仅在写入其操作日志并且应用到 master 所有的 replica 上，此改动才被认为是 commit 状态。为了简单并且不影响正常处理来自客户端的请求，处理这些 mutation 和垃圾回收都是由一个进程来做。如果这个进程挂掉了，可以立即重启。如果 master 宕机了或者 master 磁盘损坏了，GFS 外部的基础设施会监控到异常，会用 operation log + checkpoint 重启一个新的 master。client 使用主机名而不是 ip 来连接 master，启动新的 master 之后只需要修改内部的 DNS 将主机名指向新的 master就可以了。

另外 GFS 还提供了 “shadow” master，它可以在 primary 宕机时提供只读访问。这个 master 是影子而不是镜像，因此数据会相比 primary master 有延迟（通常是几分之一秒）。shadow master
 可以提高读取能力，适用于那些不太改动的文件或者不介意读取到 stale data 的客户端。事实上，因为文件的内容是从 chunkserver 上读取的，application 不会读到过期的文件内容。在短时间内过期的内容是文件的 metadata，例如：目录内容或者访问控制信息。
 
 shawdow master 读取操作日志，并且将其应用到自己本地的 metadata 中，以此来保持和 primary 的数据同步。像 primary master 一样，shawdow master 也会在启动时轮训所有的 chunkserver （后续会周期性的继续 heartbeat）来记录所有 chunk replica 的地址并且交换信息监控 chunkserver 的状态。但是所有的决策操作例如 replica 地址的更新（create or delete replica）还是由 primary 来做。
 
### Data Integrity
 
每一个 chunkserver 使用 checksumming 来检测本地数据的损坏。一个 GFS 集群通常会有数百台机器数千块磁盘，读取和写入是遇到损坏是常有的事。我们可以使用其他副本的 chunk 数据来恢复数据，通过跨 chunkserver 间的数据比较来检测故障时不现实的, 此外两个副本之间的数据不一致也是合法的。因此每个 chunkserver 需要根据自己维护的 checksum 来独立的验证自己副本数据的完整性。

每个 chunk 都被分成了 64KB 的块。每个块都有一个对应的 32 位的校验和。这些校验和和其他的 metadata 一样被存储在内存中，通过操作日志持久化，并且这些数据和用户数据时分开的。

**读取流程：**

client or 其他的 chunkserver 读取数据时，chunkserver 会验证读取所涉及到所有 block 的 checksum。如果某个块计算出来的 checksum 和存储在内存中的 checksum 不一致，当前的 chunkserver 就会给请求方返回错误并同时把自己的错误报告给 master。请求方会转而从其他的副本上读取数据，同时 master 也会从其他的 replica 上 clone 一个新的 replica。新的 replica 准备就绪之后，master 会通知旧的 replica 删除其损坏的 chunk。

**对读取的性能影响**

校验和对读取性能影响不大有几个原因：

* 大多数读取通常只是跨越几个 blocks，校验时只需要额外读取相对很少的数据。
* 同时 GFS 的客户端还会通过将读取的边界和校验和的边检对齐来减少额外开销。
* checksum 的查找和比较不需要额外的 I/O，在数据读取时，checksum 已经完成了计算，也不需要附加的 I/O 操作。

**对写入性能的影响**

因为写入大部分都是文件的追加，GFS 在追加时进行了一些 checksum 计算上的优化。

对于 append，只需要计算 append 涉及的 block 的 checksum，然后更新它。如果末尾的数据块已经发生损坏，我们没有检测到，这个 block 的损坏可以在这个 block 再次被读取的时候检测到。

对于指定偏移量的写操作，需要读取验证这个写入涉及的第一个和最后一个快的 checksum，然后再执行写操作，最后再计算并记录这些块的 checksum。如果我们在覆盖写之前不验证第一个和最后一个 block 的 checksum，有可能在写入之前数据已经损坏，那么新的写入之后重新计算一个新的 checksum，会隐藏之前数据已经损坏的事实。

在机器空闲的时候，chunkserver 会周期性的对不活跃的 chunk （读取频率比较低）进行校验。这样可以检测到这些不活跃 chunk 的数据损坏问题。一旦检测到，会上报给 master，然后 master 会创建新的 chunk，并将已经损坏的 chunk 删除。
 
### Diagnostic Tools

额外的详细的检测日志帮助我们定位分析问题，进行性能分析，这些日志的记录只需要额外很小的开销。如果没有日志事后很难理解机器间这些短暂的、一次性的交互。GFS 记录一些重要的事件（像 chunkserver 的增加或者减少）和所有的 rpc 请求和响应到日志中。我们会在空间允许的情况下尽可能多的保留这些日志。

通过匹配 RPC 日志可以重建完整的交互历史来分析问题。日志也可以帮助我们进行性能分析和负载测试。

记录日志对性能的影响是很小的，因为所有的日志都是异步的顺序记录下来的。最新的日志会存在内存中供实时的在线分析使用。


## 小结

GFS 是弱一致性系统，它只保证某个 record 在各个 chunkserver 的某个确定的 offset 上一定有（有一个没有写入成功客户端都会重试直至写成功），但不保证各个 chunkserver 上的内容完全一致，这里提几个把 GFS 设计成强一致可能需要考虑的问题：

1. 每个 chunkserver 需要检测重复请求，如果同一个请求已经执行过了，则不再执行
2. 当 primary 要求 secondary 追加数据时候，secondary 需要小心不要将数据暴露给客户端，直到 primary 确定所有的 secondary 都能够执行追加操作为止，保证所有的 client 在各个 chunkserver 上读到的内容都一样
3. 当 primary 要求 secondary 追加数据时，你能追加吗？此时 secondaries 并不是真正执行，而是缓存到本地，只有所有的 secondaries 都回复说自己能执行了，primary 通知他们一起执行（两阶段提交）


几个问题：

* 客户端怎么做 checksum
* 前缀索引 命名空间树
* 删除某个文件的时候 会先删除 master 的文件，将其重命名，这个文件具体存储的是什么内容，master 可能会存储 namespace 到 chunkIds 的映射，这个可能存储在一个文件中？如果重命名了此文件，就找不到此文件的映射关系了？
* data Integrity 中一节有说，chunkserver 中的 checksum 是以操作日志的方式进行持久化的，操作日志持久化湿指什么？checksum data 和 user data 时分开的，为什么要分开呢？


## Reference

1. [https://www.cnblogs.com/fxjwind/archive/2012/07/17/2595494.html](https://www.cnblogs.com/fxjwind/archive/2012/07/17/2595494.html)
2. [https://stackoverflow.com/questions/27864495/google-file-system-consistency-model](https://stackoverflow.com/questions/27864495/google-file-system-consistency-model)
3. [https://pdos.csail.mit.edu/6.824/papers/gfs.pdf](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)
4. [https://zhuanlan.zhihu.com/p/79746847](https://zhuanlan.zhihu.com/p/79746847)
5. [https://pdos.csail.mit.edu/6.824/papers/gfs-faq.txt](https://pdos.csail.mit.edu/6.824/papers/gfs-faq.txt)
6. [https://pdos.csail.mit.edu/6.824/notes/l-gfs.txt](https://pdos.csail.mit.edu/6.824/notes/l-gfs.txt)